<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.87.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon.ico" />


<title>Optimization Algorithms M2W2 - ps126.5</title>




<meta name="keywords" content="coursera, neural network" />


<meta property="og:title" content="Optimization Algorithms M2W2" />
<meta name="twitter:title" content="Optimization Algorithms M2W2" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tjdoc.github.io/posts/2021-01-26-coursera_m2w2/" /><meta property="og:description" content="mini batch gradient descent 1 epoch: pass through training set. In this case, 5000 gradient decsents.
training with mini batch gradient descent Mini batch Cost function is much more noisier than the batch gradient descent, but nonetheless the overall trend will decrease.
choosing your mini-batch size  mini-batch size: m \(\implies\) Batch gradient descent  Smooth convergence of the cost Takes too long per iteration   mini-batch size: 1 \(\implies\) Stochastic gradient descent  Does not completely converge (noisy cost minimization) Loses speedup from vectorization   mini-batch size: between 1 and m \(\implies\) best performance in practice  Fastest learning Vectorization (~1000 batch size) Make progress without passing the entire training set    Guidelines for choosing your mini-batch size" />
<meta name="twitter:description" content="mini batch gradient descent 1 epoch: pass through training set. In this case, 5000 gradient decsents.
training with mini batch gradient descent Mini batch Cost function is much more noisier than the batch gradient descent, but nonetheless the overall trend will decrease.
choosing your mini-batch size  mini-batch size: m \(\implies\) Batch gradient descent  Smooth convergence of the cost Takes too long per iteration   mini-batch size: 1 \(\implies\) Stochastic gradient descent  Does not completely converge (noisy cost minimization) Loses speedup from vectorization   mini-batch size: between 1 and m \(\implies\) best performance in practice  Fastest learning Vectorization (~1000 batch size) Make progress without passing the entire training set    Guidelines for choosing your mini-batch size" /><meta name="twitter:card" content="summary" /><meta property="article:published_time" content="2021-01-26T10:29:44+09:00" /><meta property="article:modified_time" content="2021-01-26T10:29:44+09:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>



<link rel="stylesheet" href="https://tjdoc.github.io/assets/css/fuji.min.css" />





</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>
    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://tjdoc.github.io/">ps126.5</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-26-coursera_m2w2/">Optimization Algorithms M2W2</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-26</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h2 id="mini-batch-gradient-descent">mini batch gradient descent</h2>
<p><img class="img-zoomable" src="/coursera_dl/minibatch.png" alt="mini batch" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/minibatch2.png" alt="mini batch2" />
</p>
<p>1 <strong>epoch</strong>: pass through training set. In this case, 5000 gradient decsents.</p>
<h2 id="training-with-mini-batch-gradient-descent">training with mini batch gradient descent</h2>
<p><img class="img-zoomable" src="/coursera_dl/minibatch3.jpeg" alt="mini batch3" />
</p>
<p>Mini batch Cost function is much more noisier than the batch gradient descent, but nonetheless the overall trend will decrease.</p>
<h2 id="choosing-your-mini-batch-size">choosing your mini-batch size</h2>
<p><img class="img-zoomable" src="/coursera_dl/minibatch4.jpeg" alt="mini batch4" />
</p>
<ul>
<li>mini-batch size: m \(\implies\) Batch gradient descent
<ul>
<li>Smooth convergence of the cost</li>
<li>Takes too long per iteration</li>
</ul>
</li>
<li>mini-batch size: 1 \(\implies\) Stochastic gradient descent
<ul>
<li>Does not completely converge (noisy cost minimization)</li>
<li>Loses speedup from vectorization</li>
</ul>
</li>
<li>mini-batch size: between 1 and m \(\implies\) best performance in practice
<ul>
<li>Fastest learning</li>
<li>Vectorization (~1000 batch size)</li>
<li>Make progress without passing the entire training set</li>
</ul>
</li>
</ul>
<p>Guidelines for choosing your mini-batch size</p>
<ul>
<li>If you have small training set (less than 2000): Use Batch gradient descent</li>
<li>Typical mini-batch sizes (power of 2)
<ul>
<li>64</li>
<li>128</li>
<li>256</li>
<li>512</li>
<li>1024</li>
</ul>
</li>
<li>Make sure mini-batch \(X^{{t}}, Y^{{t}}\) fits in your CPU and GPU</li>
</ul>
<h2 id="exponentially-weighted-averages">Exponentially weighted averages</h2>
<p><img class="img-zoomable" src="/coursera_dl/expwavg.jpeg" alt="exponentially weighted averages" />
</p>
<p><strong>Exponentially weighted moving average</strong></p>
<p>$$ V_t = \beta V_{t-1} + (1-\beta) \theta_t $$
\(V_t\) as approximately average over \(\frac{1}{1-\beta}\) days</p>
<p>$$
\begin{cases}
\beta = 0.9 &amp;\approx 10 \text{ days temperature} \\
\beta = 0.98 &amp;\approx 50 \text{ days temperature} \\
\beta = 0.5 &amp;\approx 2 \text{ days temperature}
\end{cases}
$$</p>
<p><img class="img-zoomable" src="/coursera_dl/expwavg2.jpeg" alt="exponentially weighted averages2" />
</p>
<p>notice that with larger averaging, there is a right shift tendency.</p>
<h2 id="implementing-exponentially-weighted-averages">implementing exponentially weighted averages</h2>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">v_theta = <span style="color:#ff0;font-weight:bold">0</span>
<span style="color:#fff;font-weight:bold">for</span> ii in <span style="color:#fff;font-weight:bold">range</span>(iteration):
    v_theta = beta * v_theta + (<span style="color:#ff0;font-weight:bold">1</span>-beta)*theta_t
</code></pre></div><ul>
<li>Pro: less memory needed</li>
<li>Con: gives you less accurate estimate of the moving average.</li>
</ul>
<p><img class="img-zoomable" src="/coursera_dl/expwavg3.png" alt="exp avg example" />
</p>
<p>you get the idea</p>
<h2 id="bias-correction">Bias correction</h2>
<p>The exponentially weighted moving average has a bias in the beginning because v_theta is initialized with zero.</p>
<p><img class="img-zoomable" src="/coursera_dl/biasCorrection.jpeg" alt="bias correction" />
</p>
<p>I don&rsquo;t understand this logic</p>
<h2 id="gradient-descent-with-momentum">gradient descent with momentum</h2>
<p><img class="img-zoomable" src="/coursera_dl/gdmomentum.jpeg" alt="momentum" />
</p>
<h2 id="rms-prop">rms prop</h2>
<p><img class="img-zoomable" src="/coursera_dl/rmsprop.jpeg" alt="rms prop" />
</p>
<h2 id="adam-optimization-algorithm">Adam optimization algorithm</h2>
<p><img class="img-zoomable" src="/coursera_dl/adamoptimization.jpeg" alt="adam optimizaiton" />
</p>
<p>correction</p>
<p>$$ S_{db} = \beta_2 S_{db} + (1-\beta_2) db^2 $$</p>
<p>the square is missing in the figure</p>
<h3 id="adam-optimization-hyperparameter-choice">Adam optimization hyperparameter choice</h3>
<p>adam: adaptive moment estimation</p>
<ul>
<li>alpma: needs to be tuned</li>
<li>beta 1: 0.9 (dw)</li>
<li>beta 2: 0.999 (dw^2)</li>
<li>epsilon: 10^(-8)</li>
</ul>
<h2 id="learning-rate-decay">learning rate decay</h2>
<table>
<thead>
<tr>
<th>Epoch</th>
<th>\(\alpha\)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.1</td>
</tr>
<tr>
<td>2</td>
<td>0.067</td>
</tr>
<tr>
<td>3</td>
<td>0.05</td>
</tr>
<tr>
<td>4</td>
<td>0.04</td>
</tr>
</tbody>
</table>
<p>$$ \alpha = \frac{1}{1+decayRate \times epochNum} \alpha_0 $$</p>
<h2 id="other-learning-rate-decay-methods">other learning rate decay methods</h2>
<p><img class="img-zoomable" src="/coursera_dl/otherDecay.png" alt="other decay" />
</p>
<h2 id="the-problem-of-local-optima">the problem of local optima</h2>
<p><img class="img-zoomable" src="/coursera_dl/localOptima.jpeg" alt="local optima" />
</p>
<h2 id="problem-of-plateaus">problem of plateaus</h2>
<p><img class="img-zoomable" src="/coursera_Dl/plateaus.jpeg" alt="plateau" />
</p>

    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/cnn/">cnn</a>
            </span>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/neural-networks/">neural networks</a>
            </span>
            
            <span>
                <a href="/tags/optimization/">optimization</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
            <span>
                <a href="/tags/regularization/">regularization</a>
            </span>
            
            <span>
                <a href="/tags/tensorflow/">tensorflow</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>TOC</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#mini-batch-gradient-descent">mini batch gradient descent</a></li>
    <li><a href="#training-with-mini-batch-gradient-descent">training with mini batch gradient descent</a></li>
    <li><a href="#choosing-your-mini-batch-size">choosing your mini-batch size</a></li>
    <li><a href="#exponentially-weighted-averages">Exponentially weighted averages</a></li>
    <li><a href="#implementing-exponentially-weighted-averages">implementing exponentially weighted averages</a></li>
    <li><a href="#bias-correction">Bias correction</a></li>
    <li><a href="#gradient-descent-with-momentum">gradient descent with momentum</a></li>
    <li><a href="#rms-prop">rms prop</a></li>
    <li><a href="#adam-optimization-algorithm">Adam optimization algorithm</a>
      <ul>
        <li><a href="#adam-optimization-hyperparameter-choice">Adam optimization hyperparameter choice</a></li>
      </ul>
    </li>
    <li><a href="#learning-rate-decay">learning rate decay</a></li>
    <li><a href="#other-learning-rate-decay-methods">other learning rate decay methods</a></li>
    <li><a href="#the-problem-of-local-optima">the problem of local optima</a></li>
    <li><a href="#problem-of-plateaus">problem of plateaus</a></li>
  </ul>
</nav></div>
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/cnn/">cnn</a>
            </span>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/neural-networks/">neural networks</a>
            </span>
            
            <span>
                <a href="/tags/optimization/">optimization</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
            <span>
                <a href="/tags/regularization/">regularization</a>
            </span>
            
            <span>
                <a href="/tags/tensorflow/">tensorflow</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2021
                <a href="https://tjdoc.github.io/"></a>
                
                | Powered by <a href="https://github.com/amzrk2/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.0/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"
></script>



</body>

</html>
<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.87.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon.ico" />


<title>Improving Deep Neural Networks M2W3 - ps126.5</title>




<meta name="keywords" content="coursera, neural networks, hyperparameter tuning, regularization" />


<meta property="og:title" content="Improving Deep Neural Networks M2W3" />
<meta name="twitter:title" content="Improving Deep Neural Networks M2W3" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tjdoc.github.io/posts/2021-01-28-coursera_m2w3/" /><meta property="og:description" content="Hyperparameters in the order of importance
 alpha - most important   beta (momentum, default: 0.9) number of hidden units mini-batch size   number of layers learning rate decay beta1, beta2, epsilon (Adam optimization, 0.9, 0.999, 10^-8)  Tuning process  Try random sampling, don&rsquo;t use grid  it is difficult to know in advance which setting will work better   use coarse to fine sampling once you have some idea which range works better - sample that region more densely  picking hyperparameters at random  uniformly random (random version of np." />
<meta name="twitter:description" content="Hyperparameters in the order of importance
 alpha - most important   beta (momentum, default: 0.9) number of hidden units mini-batch size   number of layers learning rate decay beta1, beta2, epsilon (Adam optimization, 0.9, 0.999, 10^-8)  Tuning process  Try random sampling, don&rsquo;t use grid  it is difficult to know in advance which setting will work better   use coarse to fine sampling once you have some idea which range works better - sample that region more densely  picking hyperparameters at random  uniformly random (random version of np." /><meta name="twitter:card" content="summary" /><meta property="article:published_time" content="2021-01-28T18:19:55+09:00" /><meta property="article:modified_time" content="2021-01-28T18:19:55+09:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>



<link rel="stylesheet" href="https://tjdoc.github.io/assets/css/fuji.min.css" />





</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>
    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://tjdoc.github.io/">ps126.5</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-28-coursera_m2w3/">Improving Deep Neural Networks M2W3</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-28</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-networks">neural networks</a>&nbsp;<a href="/tags/hyperparameter-tuning">hyperparameter tuning</a>&nbsp;<a href="/tags/regularization">regularization</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h2 id="hyperparameters">Hyperparameters</h2>
<p>in the order of importance</p>
<ul>
<li>alpha - most important</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>beta (momentum, default: 0.9)</li>
<li>number of hidden units</li>
<li>mini-batch size</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>number of layers</li>
<li>learning rate decay</li>
<li>beta1, beta2, epsilon (Adam optimization, 0.9, 0.999, 10^-8)</li>
</ul>
<h2 id="tuning-process">Tuning process</h2>
<ul>
<li><strong>Try random sampling</strong>, don&rsquo;t use grid
<ul>
<li>it is difficult to know in advance which setting will work better</li>
</ul>
</li>
<li>use <strong>coarse to fine sampling</strong> once you have some idea which range works better - sample that region more densely</li>
</ul>
<h3 id="picking-hyperparameters-at-random">picking hyperparameters at random</h3>
<ul>
<li>uniformly random (random version of np.linspace)</li>
<li>appropriate scale for hyperparameters (random version of np.logspace)</li>
</ul>
<p><img class="img-zoomable" src="/coursera_dl/logscale_sampling.jpeg" alt="logscale sampling" />
</p>
<ul>
<li>hyperparameters for exponentially weighted averages
<ul>
<li>0.9, 0.99, 0.999&hellip;</li>
</ul>
</li>
</ul>
<p><img class="img-zoomable" src="/coursera_dl/expw_hyper.jpeg" alt="hyper expon" />
</p>
<h2 id="hyperparameters-tuning-in-practice-pandas-vs-caviar">Hyperparameters tuning in practice: Pandas vs. Caviar</h2>
<ul>
<li>Pandas: Babysiting one model</li>
<li>Caviar: training many models in parallel</li>
</ul>
<p><img class="img-zoomable" src="/coursera_dl/pandas_caviar.jpeg" alt="pandas caviar" />
</p>
<h2 id="normalizing-activations">normalizing activations</h2>
<p><img class="img-zoomable" src="/coursera_dl/normalizing_inputs.jpeg" alt="normalizing" />
</p>
<h2 id="implemening-batch-norm">implemening batch norm</h2>
<p><img class="img-zoomable" src="/coursera_dl/batch_norm.jpeg" alt="batch norm" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/batch_norm_network.jpeg" alt="batch_norm" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/batch_norm_minibatch.jpeg" alt="batch norm minibatch" />
</p>
<h2 id="implementing-gradient-descent">implementing gradient descent</h2>
<p><img class="img-zoomable" src="/coursera_dl/batch_norm_gd.jpeg" alt="batch norm gradient descent" />
</p>
<h2 id="why-does-batch-norm-work">why does batch norm work?</h2>
<p>it weakens the coupling between each layers - learning can be a little more indepenant because mean and variance is mormalized</p>
<p><img class="img-zoomable" src="/coursera_dl/why_batchnorm_work.jpeg" alt="why" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/why_batchnorm_work2.jpeg" alt="why" />
</p>
<p>if you use a larger mini-batch size, you are reducing the regularization effect of the batch norm.</p>
<h2 id="batch-norm-at-test-time">batch norm at test time</h2>
<p><img class="img-zoomable" src="/coursera_dl/batch_norm_at_test.jpeg" alt="test time" />
</p>
<h2 id="multiclass-classification---softmax-regression">Multiclass classification - softmax regression</h2>
<p><img class="img-zoomable" src="/coursera_dl/softmax1.jpeg" alt="softmax 1" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/softmax2.jpeg" alt="softmax 2" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/softmax3.jpeg" alt="softmax 3" />
</p>
<h2 id="training-a-softmax-classifier">training a softmax classifier</h2>
<p>softmax regression generalizes logistic regression to C classes. If C = 2, softmax reduces to logistic regression.</p>
<p><img class="img-zoomable" src="/coursera_dl/softmax4.jpeg" alt="softmax 4" />
</p>
<h2 id="loss-function">loss function</h2>
<p><img class="img-zoomable" src="/coursera_dl/softmax5.jpeg" alt="softmax 5" />
</p>
<h2 id="gradient-descent-with-softmax">gradient descent with softmax</h2>
<p><img class="img-zoomable" src="/coursera_dl/softmax6.png" alt="softmax 6" />
</p>

    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/cnn/">cnn</a>
            </span>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/neural-networks/">neural networks</a>
            </span>
            
            <span>
                <a href="/tags/optimization/">optimization</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
            <span>
                <a href="/tags/regularization/">regularization</a>
            </span>
            
            <span>
                <a href="/tags/tensorflow/">tensorflow</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>TOC</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#hyperparameters">Hyperparameters</a></li>
    <li><a href="#tuning-process">Tuning process</a>
      <ul>
        <li><a href="#picking-hyperparameters-at-random">picking hyperparameters at random</a></li>
      </ul>
    </li>
    <li><a href="#hyperparameters-tuning-in-practice-pandas-vs-caviar">Hyperparameters tuning in practice: Pandas vs. Caviar</a></li>
    <li><a href="#normalizing-activations">normalizing activations</a></li>
    <li><a href="#implemening-batch-norm">implemening batch norm</a></li>
    <li><a href="#implementing-gradient-descent">implementing gradient descent</a></li>
    <li><a href="#why-does-batch-norm-work">why does batch norm work?</a></li>
    <li><a href="#batch-norm-at-test-time">batch norm at test time</a></li>
    <li><a href="#multiclass-classification---softmax-regression">Multiclass classification - softmax regression</a></li>
    <li><a href="#training-a-softmax-classifier">training a softmax classifier</a></li>
    <li><a href="#loss-function">loss function</a></li>
    <li><a href="#gradient-descent-with-softmax">gradient descent with softmax</a></li>
  </ul>
</nav></div>
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/cnn/">cnn</a>
            </span>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/neural-networks/">neural networks</a>
            </span>
            
            <span>
                <a href="/tags/optimization/">optimization</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
            <span>
                <a href="/tags/regularization/">regularization</a>
            </span>
            
            <span>
                <a href="/tags/tensorflow/">tensorflow</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2021
                <a href="https://tjdoc.github.io/"></a>
                
                | Powered by <a href="https://github.com/amzrk2/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.0/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>


</body>

</html>
<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.83.1" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon.ico" />


<title>Neural Networks and Deep Learning W3: Shallow Neural Network - ps126.5</title>



<meta name="description" content="A smalll description" />


<meta name="keywords" content="coursera, neural network" />


<meta property="og:title" content="Neural Networks and Deep Learning W3: Shallow Neural Network" />
<meta name="twitter:title" content="Neural Networks and Deep Learning W3: Shallow Neural Network" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/" /><meta property="og:description" content="A smalll description" />
<meta name="twitter:description" content="A smalll description" /><meta name="twitter:card" content="summary" /><meta property="article:published_time" content="2021-01-19T10:27:23+09:00" /><meta property="article:modified_time" content="2021-01-19T10:27:23+09:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>



<link rel="stylesheet" href="https://tjdoc.github.io/assets/css/fuji.min.css" />





</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>
    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://tjdoc.github.io/">ps126.5</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/">Neural Networks and Deep Learning W3: Shallow Neural Network</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-19</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <h2 id="2-layer-neural-network">2 layer neural network</h2>
<p><img class="img-zoomable" src="/coursera_dl/2layernn.jpeg" alt="shallow2lnn" />
</p>
<p>Unlike logistic regression, neural network contains hidden layer(s). The figure above depicts a shallow neural network, 2 layer NN to be precise. The input layer (layer zero) does not count as a layer so the two layers are 1 hidden layer and the output layer. With the hidden layer, layer numbers are identified by the bracket superscript.</p>
<p><img class="img-zoomable" src="/coursera_dl/2layernn1.jpeg" alt="shallow2lnn1" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/2layernn2.jpeg" alt="shallow2lnn2" />
</p>
<h2 id="vectorizing-across-multiple-examples">vectorizing across multiple examples</h2>
<p><img class="img-zoomable" src="/coursera_dl/2layernn3.jpeg" alt="shallow2lnn3" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/2layernn4.jpeg" alt="shallow2lnn4" />
</p>
<p><img class="img-zoomable" src="/coursera_dl/2layernn5.jpeg" alt="shallow2lnn5" />
</p>
<h2 id="activation-functions">activation functions</h2>
<p><img class="img-zoomable" src="/coursera_dl/activationFunctions.svg" alt="activation function" />
</p>
<p>If you don&rsquo;t know which activation function works best, test them.</p>
<h3 id="sigmoid-function">sigmoid function</h3>
<p>Never use this unless using for binary classification for your output layer</p>
<p>Almost always, the tanh is superior.</p>
<p>$$ \sigma(z) = \frac{1}{1+e^{-z}} $$</p>
<h3 id="hypertangent">hypertangent</h3>
<p>$$ a = \tanh (z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} $$</p>
<h3 id="relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</h3>
<p>Default</p>
<p>$$ a = np.max(0, z) $$</p>
<p>Non-differentialbility at z=0 in practice does not pose any problem.</p>
<h3 id="leaky-relu">leaky ReLU</h3>
<p>$$ a = max(az, z) $$
where a is a small positive number less than 1 (e.g., 0.01)</p>
<p>ReLU or leaky ReLU works better (learns faster) than sigmoid because the slope of the function at large z is greater than sigmoid.</p>
<h2 id="why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</h2>
<p>if you don&rsquo;t use non-linear activation functions and use</p>
<p>$$ a = z $$</p>
<p>which is know as <em>linear</em> activation function, or <em>identity</em> activation function, it turns out that the model jest compute \(a\) as a <em>linear function of your input features</em>. If you use linear activation function, no matter how many layers you use, it ends up of a linear combination of the input feature, the model is no more expressive than a linear regression model.</p>
<h2 id="derivatives-of-activation-functions">Derivatives of activation functions</h2>
<p>Sigmoid</p>
<p>$$
\begin{aligned}
g(z) &amp;= \frac{1}{1+e^{-z}} \\
g'(z) &amp;= g(z)(1-g(z))
\end{aligned}
$$</p>
<p>Tanh</p>
<p>$$
\begin{aligned}
g(z) &amp;= \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\
g'(z) &amp;= 1-g(z)^2
\end{aligned}
$$</p>
<p>ReLU</p>
<p>$$
\begin{aligned}
g(z) &amp;= \max(0, z) \\
g'(z) &amp;= \begin{cases}
0 &amp;\text{if } &amp;x \lt 0 \\
1 &amp;\text{if } &amp;x \ge 0 \\
\end{cases}
\end{aligned}
$$</p>
<p>Leaky ReLU</p>
<p>$$
\begin{aligned}
g(z) &amp;= \max(0.001*z, z) \\
g'(z) &amp;= \begin{cases}
0.001 &amp;\text{if } &amp;x \lt 0 \\
1 &amp;\text{if } &amp;x \ge 0 \\
\end{cases}
\end{aligned}
$$</p>
<h2 id="random-initialization">Random initialization</h2>
<p>$$ w^{[1]} = np.random.randn((2,2))*0.01 $$
$$ b^{[1]} = np.zero((2,1)) $$</p>
<p>Randomization of w is needed or we will be unable to update the function. Random value for b is not needed.
Multiplying 0.01 is needed because in case you use tanh or sigmoid activation functions, large z will result in very small slope of the activation function, causing very slow learning for gradient descent. If you don&rsquo;t have sigmoid or tanh, it is less of an issue but if you&rsquo;re doing binary classification, your output layer may be using them.</p>

    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/bible/">bible</a>
            </span>
            
            <span>
                <a href="/tags/cnn/">cnn</a>
            </span>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/neural-networks/">neural networks</a>
            </span>
            
            <span>
                <a href="/tags/optimization/">optimization</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
            <span>
                <a href="/tags/regularization/">regularization</a>
            </span>
            
            <span>
                <a href="/tags/tensorflow/">tensorflow</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>TOC</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#2-layer-neural-network">2 layer neural network</a></li>
    <li><a href="#vectorizing-across-multiple-examples">vectorizing across multiple examples</a></li>
    <li><a href="#activation-functions">activation functions</a>
      <ul>
        <li><a href="#sigmoid-function">sigmoid function</a></li>
        <li><a href="#hypertangent">hypertangent</a></li>
        <li><a href="#relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</a></li>
        <li><a href="#leaky-relu">leaky ReLU</a></li>
      </ul>
    </li>
    <li><a href="#why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</a></li>
    <li><a href="#derivatives-of-activation-functions">Derivatives of activation functions</a></li>
    <li><a href="#random-initialization">Random initialization</a></li>
  </ul>
</nav></div>
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/bible/">bible</a>
            </span>
            
            <span>
                <a href="/tags/cnn/">cnn</a>
            </span>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/neural-networks/">neural networks</a>
            </span>
            
            <span>
                <a href="/tags/optimization/">optimization</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
            <span>
                <a href="/tags/regularization/">regularization</a>
            </span>
            
            <span>
                <a href="/tags/tensorflow/">tensorflow</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2021
                <a href="https://tjdoc.github.io/"></a>
                
                | Powered by <a href="https://github.com/amzrk2/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.0/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"
></script>



</body>

</html>
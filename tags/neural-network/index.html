<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.80.0" />


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon.ico" />


<title>neural network - ps126.5</title>





<meta property="og:title" content="neural network" />
<meta name="twitter:title" content="neural network" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://tjdoc.github.io/tags/neural-network/" /><meta name="twitter:card" content="summary" /><meta property="og:updated_time" content="2021-01-25T16:53:28+09:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>



<link rel="stylesheet" href="https://tjdoc.github.io/assets/css/fuji.min.css" />





</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>
    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://tjdoc.github.io/">ps126.5</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                

<div class="page-info">
    <span>Posts with the tag neural network: </span>
</div>



<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-25-coursera_m2w1p/">coursera m2w1p</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-25</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Initialization init util import numpy as np import matplotlib.pyplot as plt import h5py import sklearn import sklearn.datasets def sigmoid(x): &#34;&#34;&#34; Compute the sigmoid of x Arguments: x -- A scalar or numpy array of any size. Return: s -- sigmoid(x) &#34;&#34;&#34; s = 1/(1+np.exp(-x)) return s def relu(x): &#34;&#34;&#34; Compute the relu of x Arguments: x -- A scalar or numpy array of any size. Return: s -- relu(x) &#34;&#34;&#34; s = np.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-24-coursera_m2w1/">Improving Deep Neural Networks m2w1</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-25</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;<a href="/tags/hyperparameter-tuning">hyperparameter tuning</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Train / dev / test sets Previously:
 Train: Test = 70% : 30% Train : dev : test = 60% : 20% : 20%  However, in the big data era where you have more than 1 million sets
 Train : dev : test = 1,000,000 : 10,000 : 10,000 Train : dev : test = 99.5% : 0.25% : 0.25% Train : dev : test = 99.5% : 0.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-23-m1w4p2_review/">coursera m1w4p2 review</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-23</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        python stuff learned elementwise min max for relu, you use elementwise maximum function which can be done by
np.maximum / np.minimum  np.dot for a given numpy array a and b, for a dot product, both expressions are identical
np.dot(a, b) a.dot(b)  decision boundary plot import matplotlib.pyplot as plt import numpy as np import sklearn import sklearn.datasets import sklearn.linear_model def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p2/">m1w4p2</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-22</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        import numpy as np import matplotlib.pyplot as plt import h5py def sigmoid(Z): &#34;&#34;&#34; Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation &#34;&#34;&#34; A = 1/(1+np.exp(-Z)) cache = Z return A, cache def relu(Z): &#34;&#34;&#34; Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing &#34;A&#34; ; stored for computing the backward pass efficiently &#34;&#34;&#34; A = np.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p1/">coursera m1w4 p1</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-22</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        dnn utility import numpy as np def sigmoid(Z): &#34;&#34;&#34; Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation &#34;&#34;&#34; A = 1/(1+np.exp(-Z)) cache = Z return A, cache def relu(Z): &#34;&#34;&#34; Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing &#34;A&#34; ; stored for computing the backward pass efficiently &#34;&#34;&#34; A = np.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4/">Neural Networks and Deep Learning W4: Deepl L-layer neural network</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-22</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Terminology  L number of layers \(n^{[l]}\) number of units in layer l \(a^{[l]}\) activations in layer l, \( a^{[l]} = g^{[l]} (z^{[l]}) \)  Forward propagation in deep network $$ \begin{aligned} Z^{[l]} =&amp; W^{[l]} A^{[l-1]} + b^{[l]} \\
A^{[l]} =&amp; g^{[l]} (Z^{[l]}) \end{aligned} $$
where \( A^{[0]} = X \)
You will need an explict for-loop for computing each layers.
Getting your matrix dimensions right $$ \begin{aligned} W^{[l]} &amp;: (n^{[l]}, n^{[l-1]}) \\
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-19-m1w3_code/">M1W3 Code</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-19</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Planar utils import matplotlib.pyplot as plt import numpy as np import sklearn import sklearn.datasets import sklearn.linear_model def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1 y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1 h = 0.01 # Generate a grid of points with distance h between them xx, yy = np.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/">Neural Networks and Deep Learning W3: Shallow Neural Network</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-19</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        2 layer neural network Unlike logistic regression, neural network contains hidden layer(s). The figure above depicts a shallow neural network, 2 layer NN to be precise. The input layer (layer zero) does not count as a layer so the two layers are 1 hidden layer and the output layer. With the hidden layer, layer numbers are identified by the bracket superscript.
vectorizing across multiple examples activation functions If you don&rsquo;t know which activation function works best, test them.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_lab/">Neural Networksand Deep Learning W2: Lab</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-18</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        packages import numpy as np import matplotlib.pyplot as plt import h5py import scipy from PIL import Image from scipy import ndimage from lr_utils import load_dataset %matplotlib inline load dataset import numpy as np import h5py def load_dataset(): train_dataset = h5py.File(&#39;datasets/train_catvnoncat.h5&#39;, &#34;r&#34;) train_set_x_orig = np.array(train_dataset[&#34;train_set_x&#34;][:]) # your train set features train_set_y_orig = np.array(train_dataset[&#34;train_set_y&#34;][:]) # your train set labels test_dataset = h5py.File(&#39;datasets/test_catvnoncat.h5&#39;, &#34;r&#34;) test_set_x_orig = np.array(test_dataset[&#34;test_set_x&#34;][:]) # your test set features test_set_y_orig = np.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_code/">Coursera W2 Numpy Code</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-18</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Sigmoid function import math import numpy as np def basic_sigmoid(x): return 1/(1+math.exp(-x)) def sigmoid(x) return 1/(1+np.exp(-x)) Derivative of sigmoid $$ \sigma ' (x) = \sigma(x)(1-\sigma(x)) $$
def sigmoid_derivative(x): s = sigmoid(x) ds = s*(1-s) return ds Reshaping arbitrary dimension numpy array to a column vector def image2vector(image): &#34;&#34;&#34; Argument: image -- a numpy array of shape (length, height, depth) Returns: v -- a vector of shape (length*height*depth, 1) &#34;&#34;&#34; # v = image.
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-15-coursera_m1w2/">Neural Networks and Deep Learning W2: Logistic Regression as a Neural Network</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-15</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Logistic Regression Given x, we want \({\hat y} = P(y=1 |_x)\) where
input vector: \( x \in \Re^{n_x} \)
Parameters: \(w \in \Re^{n_x}\), \(b \in \Re \)
Output: \({\hat y} = \sigma (w^T x + b)\)
Here, \( \sigma(z) = \frac{1}{1+e^{-z}} \) is the sigmoid function that enforces \( 0 \le {\hat y} \le 1 \)
Given \( \{{ (x^{(1)} , y^{(1)}) , &hellip;, (x^{(m)} , y^{(m)}) \}} \)
we want \( {\hat y}^{(i)} \approx y^{(i)} \)
    </div>
</div>

<div class="post">
    <h2 class="post-item post-title">
        <a href="https://tjdoc.github.io/posts/2021-01-14-coursera_m1w1/">Neural Networks and Deep Learning W1: What is a neural network?</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-01-14</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/coursera">coursera</a>&nbsp;<a href="/tags/neural-network">neural network</a>&nbsp;</span>

    </div>
    <div class="post-item post-summary markdown-body">
        Terminology  x: input y: output Deep Learning: Same thing as Neural Network but sounds better brandwise  Application Standard Neural Network
 Real estate Online Advertising  Convolutional NN (CNN)
 Image data Photo tagging  Recurrent NN (RNN)
 Data invoving temporal component Speech recognition Machine transaltion  Custom Hybrid
 Automous driving  Data kinds  Supervised Data: Database Unstructured Data  audio image text    Why is deep learning taking off?
    </div>
</div>










            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
        </div>
    </div>
    
    
    
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/coursera/">coursera</a>
            </span>
            
            <span>
                <a href="/tags/hyperparameter-tuning/">hyperparameter tuning</a>
            </span>
            
            <span>
                <a href="/tags/neural-network/">neural network</a>
            </span>
            
            <span>
                <a href="/tags/reflection/">reflection</a>
            </span>
            
        </div>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2021
                <a href="https://tjdoc.github.io/"></a>
                
                | Powered by <a href="https://github.com/amzrk2/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.0/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>


</body>

</html>
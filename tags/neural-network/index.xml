<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural network on ps126.5</title>
    <link>https://tjdoc.github.io/tags/neural-network/</link>
    <description>Recent content in neural network on ps126.5</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Jan 2021 10:29:44 +0900</lastBuildDate><atom:link href="https://tjdoc.github.io/tags/neural-network/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimization Algorithms M2W2</title>
      <link>https://tjdoc.github.io/posts/2021-01-26-coursera_m2w2/</link>
      <pubDate>Tue, 26 Jan 2021 10:29:44 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-26-coursera_m2w2/</guid>
      <description>mini batch gradient descent 1 epoch: pass through training set. In this case, 5000 gradient decsents.
training with mini batch gradient descent Mini batch Cost function is much more noisier than the batch gradient descent, but nonetheless the overall trend will decrease.
choosing your mini-batch size  mini-batch size: m \(\implies\) Batch gradient descent  Smooth convergence of the cost Takes too long per iteration   mini-batch size: 1 \(\implies\) Stochastic gradient descent  Does not completely converge (noisy cost minimization) Loses speedup from vectorization   mini-batch size: between 1 and m \(\implies\) best performance in practice  Fastest learning Vectorization (~1000 batch size) Make progress without passing the entire training set    Guidelines for choosing your mini-batch size</description>
    </item>
    
    <item>
      <title>coursera m2w1p</title>
      <link>https://tjdoc.github.io/posts/2021-01-25-coursera_m2w1p/</link>
      <pubDate>Mon, 25 Jan 2021 16:53:28 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-25-coursera_m2w1p/</guid>
      <description>Initialization init util import numpy as np import matplotlib.pyplot as plt import h5py import sklearn import sklearn.datasets def sigmoid(x): &amp;#34;&amp;#34;&amp;#34; Compute the sigmoid of x Arguments: x -- A scalar or numpy array of any size. Return: s -- sigmoid(x) &amp;#34;&amp;#34;&amp;#34; s = 1/(1+np.exp(-x)) return s def relu(x): &amp;#34;&amp;#34;&amp;#34; Compute the relu of x Arguments: x -- A scalar or numpy array of any size. Return: s -- relu(x) &amp;#34;&amp;#34;&amp;#34; s = np.</description>
    </item>
    
    <item>
      <title>Improving Deep Neural Networks m2w1</title>
      <link>https://tjdoc.github.io/posts/2021-01-24-coursera_m2w1/</link>
      <pubDate>Mon, 25 Jan 2021 10:10:50 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-24-coursera_m2w1/</guid>
      <description>Train / dev / test sets Previously:
 Train: Test = 70% : 30% Train : dev : test = 60% : 20% : 20%  However, in the big data era where you have more than 1 million sets
 Train : dev : test = 1,000,000 : 10,000 : 10,000 Train : dev : test = 99.5% : 0.25% : 0.25% Train : dev : test = 99.5% : 0.</description>
    </item>
    
    <item>
      <title>coursera m1w4p2 review</title>
      <link>https://tjdoc.github.io/posts/2021-01-23-m1w4p2_review/</link>
      <pubDate>Sat, 23 Jan 2021 20:02:24 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-23-m1w4p2_review/</guid>
      <description>python stuff learned elementwise min max for relu, you use elementwise maximum function which can be done by
np.maximum / np.minimum  np.dot for a given numpy array a and b, for a dot product, both expressions are identical
np.dot(a, b) a.dot(b)  decision boundary plot import matplotlib.pyplot as plt import numpy as np import sklearn import sklearn.datasets import sklearn.linear_model def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].</description>
    </item>
    
    <item>
      <title>m1w4p2</title>
      <link>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p2/</link>
      <pubDate>Fri, 22 Jan 2021 21:09:54 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p2/</guid>
      <description>import numpy as np import matplotlib.pyplot as plt import h5py def sigmoid(Z): &amp;#34;&amp;#34;&amp;#34; Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation &amp;#34;&amp;#34;&amp;#34; A = 1/(1+np.exp(-Z)) cache = Z return A, cache def relu(Z): &amp;#34;&amp;#34;&amp;#34; Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing &amp;#34;A&amp;#34; ; stored for computing the backward pass efficiently &amp;#34;&amp;#34;&amp;#34; A = np.</description>
    </item>
    
    <item>
      <title>coursera m1w4 p1</title>
      <link>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p1/</link>
      <pubDate>Fri, 22 Jan 2021 18:24:13 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p1/</guid>
      <description>dnn utility import numpy as np def sigmoid(Z): &amp;#34;&amp;#34;&amp;#34; Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation &amp;#34;&amp;#34;&amp;#34; A = 1/(1+np.exp(-Z)) cache = Z return A, cache def relu(Z): &amp;#34;&amp;#34;&amp;#34; Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing &amp;#34;A&amp;#34; ; stored for computing the backward pass efficiently &amp;#34;&amp;#34;&amp;#34; A = np.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W4: Deepl L-layer neural network</title>
      <link>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4/</link>
      <pubDate>Fri, 22 Jan 2021 16:13:24 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4/</guid>
      <description>Terminology  L number of layers \(n^{[l]}\) number of units in layer l \(a^{[l]}\) activations in layer l, \( a^{[l]} = g^{[l]} (z^{[l]}) \)  Forward propagation in deep network $$ \begin{aligned} Z^{[l]} =&amp;amp; W^{[l]} A^{[l-1]} + b^{[l]} \\
A^{[l]} =&amp;amp; g^{[l]} (Z^{[l]}) \end{aligned} $$
where \( A^{[0]} = X \)
You will need an explict for-loop for computing each layers.
Getting your matrix dimensions right $$ \begin{aligned} W^{[l]} &amp;amp;: (n^{[l]}, n^{[l-1]}) \\</description>
    </item>
    
    <item>
      <title>M1W3 Code</title>
      <link>https://tjdoc.github.io/posts/2021-01-19-m1w3_code/</link>
      <pubDate>Tue, 19 Jan 2021 19:12:37 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-19-m1w3_code/</guid>
      <description>Planar utils import matplotlib.pyplot as plt import numpy as np import sklearn import sklearn.datasets import sklearn.linear_model def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1 y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1 h = 0.01 # Generate a grid of points with distance h between them xx, yy = np.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W3: Shallow Neural Network</title>
      <link>https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/</link>
      <pubDate>Tue, 19 Jan 2021 10:27:23 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/</guid>
      <description>2 layer neural network Unlike logistic regression, neural network contains hidden layer(s). The figure above depicts a shallow neural network, 2 layer NN to be precise. The input layer (layer zero) does not count as a layer so the two layers are 1 hidden layer and the output layer. With the hidden layer, layer numbers are identified by the bracket superscript.
vectorizing across multiple examples activation functions If you don&amp;rsquo;t know which activation function works best, test them.</description>
    </item>
    
    <item>
      <title>Neural Networksand Deep Learning W2: Lab</title>
      <link>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_lab/</link>
      <pubDate>Mon, 18 Jan 2021 18:42:17 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_lab/</guid>
      <description>packages import numpy as np import matplotlib.pyplot as plt import h5py import scipy from PIL import Image from scipy import ndimage from lr_utils import load_dataset %matplotlib inline load dataset import numpy as np import h5py def load_dataset(): train_dataset = h5py.File(&amp;#39;datasets/train_catvnoncat.h5&amp;#39;, &amp;#34;r&amp;#34;) train_set_x_orig = np.array(train_dataset[&amp;#34;train_set_x&amp;#34;][:]) # your train set features train_set_y_orig = np.array(train_dataset[&amp;#34;train_set_y&amp;#34;][:]) # your train set labels test_dataset = h5py.File(&amp;#39;datasets/test_catvnoncat.h5&amp;#39;, &amp;#34;r&amp;#34;) test_set_x_orig = np.array(test_dataset[&amp;#34;test_set_x&amp;#34;][:]) # your test set features test_set_y_orig = np.</description>
    </item>
    
    <item>
      <title>Coursera W2 Numpy Code</title>
      <link>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_code/</link>
      <pubDate>Mon, 18 Jan 2021 14:57:04 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_code/</guid>
      <description>Sigmoid function import math import numpy as np def basic_sigmoid(x): return 1/(1+math.exp(-x)) def sigmoid(x) return 1/(1+np.exp(-x)) Derivative of sigmoid $$ \sigma &#39; (x) = \sigma(x)(1-\sigma(x)) $$
def sigmoid_derivative(x): s = sigmoid(x) ds = s*(1-s) return ds Reshaping arbitrary dimension numpy array to a column vector def image2vector(image): &amp;#34;&amp;#34;&amp;#34; Argument: image -- a numpy array of shape (length, height, depth) Returns: v -- a vector of shape (length*height*depth, 1) &amp;#34;&amp;#34;&amp;#34; # v = image.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W2: Logistic Regression as a Neural Network</title>
      <link>https://tjdoc.github.io/posts/2021-01-15-coursera_m1w2/</link>
      <pubDate>Fri, 15 Jan 2021 10:52:30 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-15-coursera_m1w2/</guid>
      <description>Logistic Regression Given x, we want \({\hat y} = P(y=1 |_x)\) where
input vector: \( x \in \Re^{n_x} \)
Parameters: \(w \in \Re^{n_x}\), \(b \in \Re \)
Output: \({\hat y} = \sigma (w^T x + b)\)
Here, \( \sigma(z) = \frac{1}{1+e^{-z}} \) is the sigmoid function that enforces \( 0 \le {\hat y} \le 1 \)
Given \( \{{ (x^{(1)} , y^{(1)}) , &amp;hellip;, (x^{(m)} , y^{(m)}) \}} \)
we want \( {\hat y}^{(i)} \approx y^{(i)} \)</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W1: What is a neural network?</title>
      <link>https://tjdoc.github.io/posts/2021-01-14-coursera_m1w1/</link>
      <pubDate>Thu, 14 Jan 2021 21:06:56 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-14-coursera_m1w1/</guid>
      <description>Terminology  x: input y: output Deep Learning: Same thing as Neural Network but sounds better brandwise  Application Standard Neural Network
 Real estate Online Advertising  Convolutional NN (CNN)
 Image data Photo tagging  Recurrent NN (RNN)
 Data invoving temporal component Speech recognition Machine transaltion  Custom Hybrid
 Automous driving  Data kinds  Supervised Data: Database Unstructured Data  audio image text    Why is deep learning taking off?</description>
    </item>
    
  </channel>
</rss>

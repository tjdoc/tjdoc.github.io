<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>coursera on ps126.5</title>
    <link>https://tjdoc.github.io/tags/coursera/</link>
    <description>Recent content in coursera on ps126.5</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Feb 2021 15:22:09 +0900</lastBuildDate><atom:link href="https://tjdoc.github.io/tags/coursera/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CNN m4w2</title>
      <link>https://tjdoc.github.io/posts/2021-02-02-coursera_m4w2/</link>
      <pubDate>Tue, 02 Feb 2021 15:22:09 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-02-02-coursera_m4w2/</guid>
      <description>classic networks LeNet-5 AlexNet  many similarities with LeNet5 but much bigger. (10K parameters vs 60M parameters) Uses ReLU instead of traditional sigmoid, tanh Multiple GPUs local response normalization (not used much nowadays)  VGG-16 simplified the nn architechture
ResNets networks in networks and 1x1 convolutions  pulling layers reduce nh, nw 1x1 convolution lets you reduce (or increase) number of channels  inception network </description>
    </item>
    
    <item>
      <title>convolutional neural network</title>
      <link>https://tjdoc.github.io/posts/2021-02-01-coursera_m4w1/</link>
      <pubDate>Mon, 01 Feb 2021 11:37:08 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-02-01-coursera_m4w1/</guid>
      <description>Computer vision convolution operator
summation of the elementwise multiplication with the convolution operator
 python: conv-forward tensorflow: tf.nn.conv2d keras: conv2d  padding with padding, you can
 prevent shrinking output prevent throwing away info from edge padding can be as large as you want  Valid and same convolutions  valid: no padding same: pad so that output size is the same as the input size  you may want odd number filters</description>
    </item>
    
    <item>
      <title>m2w3p tesorflow tutorial</title>
      <link>https://tjdoc.github.io/posts/2021-01-28-coursera_m2w3p/</link>
      <pubDate>Thu, 28 Jan 2021 23:52:40 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-28-coursera_m2w3p/</guid>
      <description>tf_utils.py import h5py import numpy as np import tensorflow as tf import math def load_dataset(): train_dataset = h5py.File(&amp;#39;datasets/train_signs.h5&amp;#39;, &amp;#34;r&amp;#34;) train_set_x_orig = np.array(train_dataset[&amp;#34;train_set_x&amp;#34;][:]) # your train set features train_set_y_orig = np.array(train_dataset[&amp;#34;train_set_y&amp;#34;][:]) # your train set labels test_dataset = h5py.File(&amp;#39;datasets/test_signs.h5&amp;#39;, &amp;#34;r&amp;#34;) test_set_x_orig = np.array(test_dataset[&amp;#34;test_set_x&amp;#34;][:]) # your test set features test_set_y_orig = np.array(test_dataset[&amp;#34;test_set_y&amp;#34;][:]) # your test set labels classes = np.array(test_dataset[&amp;#34;list_classes&amp;#34;][:]) # the list of classes train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.</description>
    </item>
    
    <item>
      <title>Improving Deep Neural Networks M2W3</title>
      <link>https://tjdoc.github.io/posts/2021-01-28-coursera_m2w3/</link>
      <pubDate>Thu, 28 Jan 2021 18:19:55 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-28-coursera_m2w3/</guid>
      <description>Hyperparameters in the order of importance
 alpha - most important   beta (momentum, default: 0.9) number of hidden units mini-batch size   number of layers learning rate decay beta1, beta2, epsilon (Adam optimization, 0.9, 0.999, 10^-8)  Tuning process  Try random sampling, don&amp;rsquo;t use grid  it is difficult to know in advance which setting will work better   use coarse to fine sampling once you have some idea which range works better - sample that region more densely  picking hyperparameters at random  uniformly random (random version of np.</description>
    </item>
    
    <item>
      <title>coursera m2w2p</title>
      <link>https://tjdoc.github.io/posts/2021-01-27-coursera_m2w2p/</link>
      <pubDate>Wed, 27 Jan 2021 16:16:06 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-27-coursera_m2w2p/</guid>
      <description>op utils import numpy as np import matplotlib.pyplot as plt import h5py import scipy.io import sklearn import sklearn.datasets def sigmoid(x): &amp;#34;&amp;#34;&amp;#34; Compute the sigmoid of x Arguments: x -- A scalar or numpy array of any size. Return: s -- sigmoid(x) &amp;#34;&amp;#34;&amp;#34; s = 1/(1+np.exp(-x)) return s def relu(x): &amp;#34;&amp;#34;&amp;#34; Compute the relu of x Arguments: x -- A scalar or numpy array of any size. Return: s -- relu(x) &amp;#34;&amp;#34;&amp;#34; s = np.</description>
    </item>
    
    <item>
      <title>Optimization Algorithms M2W2</title>
      <link>https://tjdoc.github.io/posts/2021-01-26-coursera_m2w2/</link>
      <pubDate>Tue, 26 Jan 2021 10:29:44 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-26-coursera_m2w2/</guid>
      <description>mini batch gradient descent 1 epoch: pass through training set. In this case, 5000 gradient decsents.
training with mini batch gradient descent Mini batch Cost function is much more noisier than the batch gradient descent, but nonetheless the overall trend will decrease.
choosing your mini-batch size  mini-batch size: m \(\implies\) Batch gradient descent  Smooth convergence of the cost Takes too long per iteration   mini-batch size: 1 \(\implies\) Stochastic gradient descent  Does not completely converge (noisy cost minimization) Loses speedup from vectorization   mini-batch size: between 1 and m \(\implies\) best performance in practice  Fastest learning Vectorization (~1000 batch size) Make progress without passing the entire training set    Guidelines for choosing your mini-batch size</description>
    </item>
    
    <item>
      <title>coursera m2w1p</title>
      <link>https://tjdoc.github.io/posts/2021-01-25-coursera_m2w1p/</link>
      <pubDate>Mon, 25 Jan 2021 16:53:28 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-25-coursera_m2w1p/</guid>
      <description>Initialization init util import numpy as np import matplotlib.pyplot as plt import h5py import sklearn import sklearn.datasets def sigmoid(x): &amp;#34;&amp;#34;&amp;#34; Compute the sigmoid of x Arguments: x -- A scalar or numpy array of any size. Return: s -- sigmoid(x) &amp;#34;&amp;#34;&amp;#34; s = 1/(1+np.exp(-x)) return s def relu(x): &amp;#34;&amp;#34;&amp;#34; Compute the relu of x Arguments: x -- A scalar or numpy array of any size. Return: s -- relu(x) &amp;#34;&amp;#34;&amp;#34; s = np.</description>
    </item>
    
    <item>
      <title>Improving Deep Neural Networks m2w1</title>
      <link>https://tjdoc.github.io/posts/2021-01-24-coursera_m2w1/</link>
      <pubDate>Mon, 25 Jan 2021 10:10:50 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-24-coursera_m2w1/</guid>
      <description>Train / dev / test sets Previously:
 Train: Test = 70% : 30% Train : dev : test = 60% : 20% : 20%  However, in the big data era where you have more than 1 million sets
 Train : dev : test = 1,000,000 : 10,000 : 10,000 Train : dev : test = 99.5% : 0.25% : 0.25% Train : dev : test = 99.5% : 0.</description>
    </item>
    
    <item>
      <title>coursera m1w4p2 review</title>
      <link>https://tjdoc.github.io/posts/2021-01-23-m1w4p2_review/</link>
      <pubDate>Sat, 23 Jan 2021 20:02:24 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-23-m1w4p2_review/</guid>
      <description>python stuff learned elementwise min max for relu, you use elementwise maximum function which can be done by
np.maximum / np.minimum  np.dot for a given numpy array a and b, for a dot product, both expressions are identical
np.dot(a, b) a.dot(b)  decision boundary plot import matplotlib.pyplot as plt import numpy as np import sklearn import sklearn.datasets import sklearn.linear_model def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].</description>
    </item>
    
    <item>
      <title>m1w4p2</title>
      <link>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p2/</link>
      <pubDate>Fri, 22 Jan 2021 21:09:54 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p2/</guid>
      <description>import numpy as np import matplotlib.pyplot as plt import h5py def sigmoid(Z): &amp;#34;&amp;#34;&amp;#34; Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation &amp;#34;&amp;#34;&amp;#34; A = 1/(1+np.exp(-Z)) cache = Z return A, cache def relu(Z): &amp;#34;&amp;#34;&amp;#34; Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing &amp;#34;A&amp;#34; ; stored for computing the backward pass efficiently &amp;#34;&amp;#34;&amp;#34; A = np.</description>
    </item>
    
    <item>
      <title>coursera m1w4 p1</title>
      <link>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p1/</link>
      <pubDate>Fri, 22 Jan 2021 18:24:13 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4p1/</guid>
      <description>dnn utility import numpy as np def sigmoid(Z): &amp;#34;&amp;#34;&amp;#34; Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation &amp;#34;&amp;#34;&amp;#34; A = 1/(1+np.exp(-Z)) cache = Z return A, cache def relu(Z): &amp;#34;&amp;#34;&amp;#34; Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing &amp;#34;A&amp;#34; ; stored for computing the backward pass efficiently &amp;#34;&amp;#34;&amp;#34; A = np.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W4: Deepl L-layer neural network</title>
      <link>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4/</link>
      <pubDate>Fri, 22 Jan 2021 16:13:24 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-22-coursera_m1w4/</guid>
      <description>Terminology  L number of layers \(n^{[l]}\) number of units in layer l \(a^{[l]}\) activations in layer l, \( a^{[l]} = g^{[l]} (z^{[l]}) \)  Forward propagation in deep network $$ \begin{aligned} Z^{[l]} =&amp;amp; W^{[l]} A^{[l-1]} + b^{[l]} \\
A^{[l]} =&amp;amp; g^{[l]} (Z^{[l]}) \end{aligned} $$
where \( A^{[0]} = X \)
You will need an explict for-loop for computing each layers.
Getting your matrix dimensions right $$ \begin{aligned} W^{[l]} &amp;amp;: (n^{[l]}, n^{[l-1]}) \\</description>
    </item>
    
    <item>
      <title>M1W3 Code</title>
      <link>https://tjdoc.github.io/posts/2021-01-19-m1w3_code/</link>
      <pubDate>Tue, 19 Jan 2021 19:12:37 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-19-m1w3_code/</guid>
      <description>Planar utils import matplotlib.pyplot as plt import numpy as np import sklearn import sklearn.datasets import sklearn.linear_model def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1 y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1 h = 0.01 # Generate a grid of points with distance h between them xx, yy = np.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W3: Shallow Neural Network</title>
      <link>https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/</link>
      <pubDate>Tue, 19 Jan 2021 10:27:23 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-19-coursera_m1w3/</guid>
      <description>2 layer neural network Unlike logistic regression, neural network contains hidden layer(s). The figure above depicts a shallow neural network, 2 layer NN to be precise. The input layer (layer zero) does not count as a layer so the two layers are 1 hidden layer and the output layer. With the hidden layer, layer numbers are identified by the bracket superscript.
vectorizing across multiple examples activation functions If you don&amp;rsquo;t know which activation function works best, test them.</description>
    </item>
    
    <item>
      <title>Neural Networksand Deep Learning W2: Lab</title>
      <link>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_lab/</link>
      <pubDate>Mon, 18 Jan 2021 18:42:17 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_lab/</guid>
      <description>packages import numpy as np import matplotlib.pyplot as plt import h5py import scipy from PIL import Image from scipy import ndimage from lr_utils import load_dataset %matplotlib inline load dataset import numpy as np import h5py def load_dataset(): train_dataset = h5py.File(&amp;#39;datasets/train_catvnoncat.h5&amp;#39;, &amp;#34;r&amp;#34;) train_set_x_orig = np.array(train_dataset[&amp;#34;train_set_x&amp;#34;][:]) # your train set features train_set_y_orig = np.array(train_dataset[&amp;#34;train_set_y&amp;#34;][:]) # your train set labels test_dataset = h5py.File(&amp;#39;datasets/test_catvnoncat.h5&amp;#39;, &amp;#34;r&amp;#34;) test_set_x_orig = np.array(test_dataset[&amp;#34;test_set_x&amp;#34;][:]) # your test set features test_set_y_orig = np.</description>
    </item>
    
    <item>
      <title>Coursera W2 Numpy Code</title>
      <link>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_code/</link>
      <pubDate>Mon, 18 Jan 2021 14:57:04 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-18-coursera_m1w2_code/</guid>
      <description>Sigmoid function import math import numpy as np def basic_sigmoid(x): return 1/(1+math.exp(-x)) def sigmoid(x) return 1/(1+np.exp(-x)) Derivative of sigmoid $$ \sigma &#39; (x) = \sigma(x)(1-\sigma(x)) $$
def sigmoid_derivative(x): s = sigmoid(x) ds = s*(1-s) return ds Reshaping arbitrary dimension numpy array to a column vector def image2vector(image): &amp;#34;&amp;#34;&amp;#34; Argument: image -- a numpy array of shape (length, height, depth) Returns: v -- a vector of shape (length*height*depth, 1) &amp;#34;&amp;#34;&amp;#34; # v = image.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W2: Logistic Regression as a Neural Network</title>
      <link>https://tjdoc.github.io/posts/2021-01-15-coursera_m1w2/</link>
      <pubDate>Fri, 15 Jan 2021 10:52:30 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-15-coursera_m1w2/</guid>
      <description>Logistic Regression Given x, we want \({\hat y} = P(y=1 |_x)\) where
input vector: \( x \in \Re^{n_x} \)
Parameters: \(w \in \Re^{n_x}\), \(b \in \Re \)
Output: \({\hat y} = \sigma (w^T x + b)\)
Here, \( \sigma(z) = \frac{1}{1+e^{-z}} \) is the sigmoid function that enforces \( 0 \le {\hat y} \le 1 \)
Given \( \{{ (x^{(1)} , y^{(1)}) , &amp;hellip;, (x^{(m)} , y^{(m)}) \}} \)
we want \( {\hat y}^{(i)} \approx y^{(i)} \)</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning W1: What is a neural network?</title>
      <link>https://tjdoc.github.io/posts/2021-01-14-coursera_m1w1/</link>
      <pubDate>Thu, 14 Jan 2021 21:06:56 +0900</pubDate>
      
      <guid>https://tjdoc.github.io/posts/2021-01-14-coursera_m1w1/</guid>
      <description>Terminology  x: input y: output Deep Learning: Same thing as Neural Network but sounds better brandwise  Application Standard Neural Network
 Real estate Online Advertising  Convolutional NN (CNN)
 Image data Photo tagging  Recurrent NN (RNN)
 Data invoving temporal component Speech recognition Machine transaltion  Custom Hybrid
 Automous driving  Data kinds  Supervised Data: Database Unstructured Data  audio image text    Why is deep learning taking off?</description>
    </item>
    
  </channel>
</rss>
